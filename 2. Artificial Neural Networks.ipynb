{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09cb8694",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377b412",
   "metadata": {},
   "source": [
    "![](Image/Image7.jpg)\n",
    "\n",
    "上圖的 neural networks 可以用以下程式碼建立 (底層的方法) ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8848abf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9696, 0.7527])\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import torch\n",
    "\n",
    "# define the input layer\n",
    "inputs_layer = torch.tensor([2., 1.])\n",
    "\n",
    "# define the weights between the inputs and the first hidden layer\n",
    "weight1 = torch.tensor([[0.45, 0.32], [-0.12, 0.29]])\n",
    "\n",
    "# define the first layer\n",
    "first_layer = torch.matmul(inputs_layer, weight1)\n",
    "\n",
    "# define the weights between the first layer and the output layer\n",
    "weight2 = torch.tensor([[0.48, -0.12], [0.64, 0.91]])\n",
    "\n",
    "# define the outputs\n",
    "outputs_layer = torch.matmul(first_layer, weight2)\n",
    "\n",
    "# print the results\n",
    "print(outputs_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1069eef2",
   "metadata": {},
   "source": [
    "**測試：將過程中的 weights 矩陣相乘看看結果是否不一樣**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b675d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4208, 0.2372],\n",
      "        [0.1280, 0.2783]])\n",
      "tensor([0.9696, 0.7527])\n"
     ]
    }
   ],
   "source": [
    "# 將過程中的 weights 矩陣相乘\n",
    "weights = torch.matmul(weight1, weight2)\n",
    "print(weights)\n",
    "\n",
    "# 計算輸出\n",
    "outputs_layer2 = torch.matmul(inputs_layer, weights)\n",
    "\n",
    "# print the results\n",
    "print(outputs_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69d7c39",
   "metadata": {},
   "source": [
    "### 線性關係\n",
    "由於矩陣相乘就代表了 linear transformation，結合在一起也代表了同時做了多個 transformation，因此上面兩者的結果是一樣的。這也代表我們實際上可以只用一層 input layer 和一層 output layer 以及連接兩者之間的 weights 就可以達到相同的效果。事實上，任何 neural network 都可以被簡化成這種沒有 hidden layer 的 network。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef23c8",
   "metadata": {},
   "source": [
    "### 非線性關係\n",
    "基本上，所有 neural networks 都可以被簡化成沒有 hidden layers 的 network。然而，這樣代表輸出層和輸入層的關係為線性關係。也就是說，這個模型只能解釋資料中的線性關係，例如：二維平面上的一條線、三維平面上的一個平面 (下圖左)。事實上，資料之間的關係不一定是線性的，有些資料間擁有非常複雜的關係，例如下圖右。因此，我們要在模型中加入一些非線性的函數，也就是 activation functions\n",
    "\n",
    "![](Image/Image8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08843c",
   "metadata": {},
   "source": [
    "## 常用的 activation functions\n",
    "\n",
    "![](Image/Image9.jpg)\n",
    "\n",
    "以及 Softmax，可以用在 multiclass classification problems 的 output layer。\n",
    "\n",
    "當加入這些 activation functions 之後，上面例子的結果就不會相同了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd13cd",
   "metadata": {},
   "source": [
    "### Implementing ReLU (Rectified Linear Unit) in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded8259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 0.])\n",
      "tensor([[2.0000, 0.0000],\n",
      "        [1.2000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import torch.nn as nn\n",
    "\n",
    "# define a ReLU function\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# define a tensor\n",
    "t1 = torch.tensor([2., -4.])\n",
    "\n",
    "# calculate the output of ReLU function\n",
    "print(relu(t1))\n",
    "\n",
    "# define another tensor\n",
    "t2 = torch.tensor([[2., -4.], [1.2, 0.]])\n",
    "\n",
    "# calculate the output of ReLU function\n",
    "print(relu(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb7960",
   "metadata": {},
   "source": [
    "### Implementing neural networks with activation functions (from the previous example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35dd4055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9696, 0.7527])\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# define a ReLU function\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# define the input layer\n",
    "inputs_layer = torch.tensor([2., 1.])\n",
    "\n",
    "# define the weights between the inputs and the first hidden layer\n",
    "weight1 = torch.tensor([[0.45, 0.32], [-0.12, 0.29]])\n",
    "\n",
    "# define the first layer\n",
    "first_layer = relu(torch.matmul(inputs_layer, weight1))\n",
    "\n",
    "# define the weights between the first layer and the output layer\n",
    "weight2 = torch.tensor([[0.48, -0.12], [0.64, 0.91]])\n",
    "\n",
    "# define the outputs\n",
    "outputs_layer = relu(torch.matmul(first_layer, weight2))\n",
    "\n",
    "# print the results\n",
    "print(outputs_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677211cf",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "訓練 neural networks 的過程：\n",
    "\n",
    "1. 隨機給 weights 一些初始值\n",
    "2. 將所有 input 的資料帶入，利用 forward propagation 計算輸出值\n",
    "3. 評估輸出值的表現\n",
    "4. 更新 weights 並重新帶入所有相同的 input 資料\n",
    "\n",
    "過程中，評估輸出值表現的方法，就是利用 loss function 來量化模型的好壞 (例如 MSE、MAE 等等)\n",
    "\n",
    "而更新 weights 就必須計算該 loss function 的 gradient (因此 loss function 必須是 differentiable)，然後乘上自定義的 learning rate ，作為 weight 的變化方向與變化量\n",
    "\n",
    "---\n",
    "\n",
    "一般來說，regression model 常用的 loss function 是 mean square error (MSE)，而 multiclass classification model 常用 Softmax cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d3d8c",
   "metadata": {},
   "source": [
    "## Softmax Cross-entropy Loss\n",
    "\n",
    "$$ P(Y = k|X = xi) = \\frac{exp(s)*k}{ \\sum_{j}\\ exp(s)*j } $$\n",
    "\n",
    "這個公式負責將 multi-class classification neural networks 輸出的數字轉換成機率。計算方法如下：\n",
    "\n",
    "假設現在有一張貓的照片，而我想要用一個 neural network 來分類這張照片是貓、車子、還是青蛙。所以我將這張照片轉換成 vector 餵給 network (有三個 output nodes)。我會得到三個數字：\n",
    "\n",
    "![](Image/Image10.jpg)\n",
    "\n",
    "由於是 multiclass classification problem，因此不能用 MSE 來計算模型表現，也不能用準確率來評估 (因為準確率不是一個可微分的函數)，因此我們要將輸出的數字轉換成機率分布。如下圖：\n",
    "\n",
    "![](Image/Image11.jpg)\n",
    "\n",
    "由於機率皆大於 0，因此第一步是將數字都帶入 exponential function，接下來 normalize 就可以得到各類別的機率。有了機率，就可以計算 cross-entropy loss。\n",
    "\n",
    "$$ CrossEntropy Loss = -ln(正確的類別的機率) $$\n",
    "\n",
    "假設我的 network 給出貓的機率為 1 (完全認出是貓)，則 $ CrossEntropy Loss = -ln(1) = 0 $，很合理。假設我的 network 給出貓的機率接近 0 (完全沒認出來)，則 $ CrossEntropy Loss = -ln(0) = 無限大 $，代表 loss 超大，必須繼續縮小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e267422f",
   "metadata": {},
   "source": [
    "## Implement Cross-entropy loss in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "403449d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0404)\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# define an output scores\n",
    "output_scores = torch.tensor([[3.2, 5.1, -1.7]])\n",
    "\n",
    "# Specify the currect class number\n",
    "true_class = torch.tensor([0])    # 0 代表第一個 element 是正確的類別\n",
    "\n",
    "# define a cross-entropy loss object\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 計算 loss\n",
    "loss = criterion(output_scores, true_class)\n",
    "\n",
    "# print\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151e850b",
   "metadata": {},
   "source": [
    "**給定另一組 output scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "079a170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0061)\n"
     ]
    }
   ],
   "source": [
    "# define an output scores\n",
    "output_scores2 = torch.tensor([[10.2, 5.1, -1.7]])\n",
    "\n",
    "# Specify the currect class number\n",
    "true_class = torch.tensor([0])    # 0 代表第一個 element 是正確的類別\n",
    "\n",
    "# define a cross-entropy loss object\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 計算 loss\n",
    "loss2 = criterion(output_scores2, true_class)\n",
    "\n",
    "# print\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893edd9",
   "metadata": {},
   "source": [
    "可以發現 loss 少多了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14c9c7",
   "metadata": {},
   "source": [
    "# Preparing a dataset in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b627d",
   "metadata": {},
   "source": [
    "這裡會使用 MNIST、CIFAR-10 這兩個 dataset 來示範 (**皆直接包含於 PyTorch 這個套件中**)。\n",
    "\n",
    "MNIST 是 computer vision 中最具代表性的資料集，包含 70000 個手寫的 digits (每個數字都有 6000 筆訓練資料以及 1000 筆測試資料)。手寫 digits 的照片都是灰階的，而且解析度為 28 * 28。\n",
    "\n",
    "CIFAR-10 是一些自然景觀的彩色照片，總共有 60000 張照片 (包含 airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck 等 10 個類別)，其中 50000 張照片是訓練集，剩下 10000 張照片是測試集。解析度為 32 * 32，且因為是彩色，所以有 RGB 三個維度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e194763",
   "metadata": {},
   "source": [
    "### 先用 CIFAR-10 這個資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c754b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import torch\n",
    "import torchvision    # a package which deals with datasets and pre-trained neural networks\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7fd556ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a transformation of images to torch tensors using torchvision transforms \n",
    "# 下載資料時就可以直接用這個定義好的 transform 物件來轉換資料\n",
    "transformCIFAR = transforms.Compose(\n",
    "    [transforms.ToTensor(),    # define a transformation of images object\n",
    "    transforms.Normalize((0.4914, 0.48216, 0.44653),    # R, G, B 的平均 (pre-computed) 用來將數據標準化\n",
    "                        (0.24703, 0.24349, 0.26159))]    # R, G, B 的標準差 (pre-computed) 用來將數據標準化\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75c7be9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# download the CIFAR-10 datasets\n",
    "training_setCIFAR = torchvision.datasets.CIFAR10(root = \"./Datasets/CIFAR-10\", train = True, download = True, transform = transformCIFAR)\n",
    "testing_setCIFAR = torchvision.datasets.CIFAR10(root = \"./Datasets/CIFAR-10\", train = False, download = True, transform = transformCIFAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f8f4997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CIFAR-10 datasets\n",
    "trainloaderCIFAR = torch.utils.data.DataLoader(training_setCIFAR, batch_size = 32, shuffle = True, num_workers = 4)\n",
    "testloaderCIFAR = torch.utils.data.DataLoader(testing_setCIFAR, batch_size = 32, shuffle = False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4967f41c",
   "metadata": {},
   "source": [
    "由於 datasets 太大，因此我們設定 load 的 **batch size** 是 32，代表一次只用 32 筆資料來更新 weights and bias。**shuffle** 代表是否打亂資料的順序，為了增加 randomness，我們設定為 True。**num_workers** 代表訓練過程要使用多少個 processors。\n",
    "\n",
    "**Inspect the data loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46061e03",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CIFAR10' object has no attribute 'train_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-0f99a86c629a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# inspect the shape of the training dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloaderCIFAR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# inspect the shape of the testing dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestloaderCIFAR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CIFAR10' object has no attribute 'train_data'"
     ]
    }
   ],
   "source": [
    "# inspect the shape of the training dataset\n",
    "print(trainloaderCIFAR.dataset.train_data.shape)\n",
    "\n",
    "# inspect the shape of the testing dataset\n",
    "print(testloaderCIFAR.dataset.test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f05b1ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "# inspect batch size\n",
    "print(testloaderCIFAR.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da6a2709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.sampler.RandomSampler object at 0x000001C28BD0C580>\n"
     ]
    }
   ],
   "source": [
    "# inspect the type of random sampler\n",
    "print(trainloaderCIFAR.sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee09f0",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset is well prepared to be used in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cbe286",
   "metadata": {},
   "source": [
    "### 再用 MNIST 這個資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dfdefa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data to torch tensors and normalize it \n",
    "transformMNIST = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307), ((0.3081)))])   # 由於 MNIST 是灰階，因此沒有像 RGB 一樣三個平均和三個標準差，這裡平均為 0.1307，標準差為 0.3081\n",
    "\n",
    "# Prepare training set and testing set\n",
    "trainsetMNIST = torchvision.datasets.MNIST(root = \"./Datasets/MNIST\", train=True, \n",
    "                                      download=True, transform=transformMNIST)\n",
    "testsetMNIST = torchvision.datasets.MNIST(root = \"./Datasets/MNIST\", train=False, \n",
    "                                     download=True, transform=transformMNIST)\n",
    "\n",
    "# Prepare training loader and testing loader\n",
    "trainloaderMNIST = torch.utils.data.DataLoader(trainsetMNIST, batch_size = 32,\n",
    "                                          shuffle = True, num_workers=8)\n",
    "testloaderMNIST = torch.utils.data.DataLoader(testsetMNIST, batch_size = 32,\n",
    "                                         shuffle = False, num_workers=8) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c50c6ac",
   "metadata": {},
   "source": [
    "**Inspect the dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ccb3885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n",
      "32 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TangKuoLin\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:62: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "C:\\Users\\TangKuoLin\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:67: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "# Compute the shape of the training set and testing set\n",
    "trainset_shapeMNIST = trainloaderMNIST.dataset.train_data.shape\n",
    "testset_shapeMNIST = testloaderMNIST.dataset.test_data.shape\n",
    "\n",
    "# Print the computed shapes\n",
    "print(trainset_shapeMNIST, testset_shapeMNIST)\n",
    "\n",
    "# Compute the size of the minibatch for training set and testing set\n",
    "trainset_batchsizeMNIST = trainloaderMNIST.batch_size\n",
    "testset_batchsizeMNIST = testloaderMNIST.batch_size\n",
    "\n",
    "# Print sizes of the minibatch\n",
    "print(trainset_batchsizeMNIST, testset_batchsizeMNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529363bc",
   "metadata": {},
   "source": [
    "The MNIST dataset is well prepared to be used in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7488afd8",
   "metadata": {},
   "source": [
    "# Training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100179aa",
   "metadata": {},
   "source": [
    "## 先用 CIFAR-10 這個資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6561b5",
   "metadata": {},
   "source": [
    "### 建立模型 (設計模型的 class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44571893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F    # Provide a more functional way than the nn module (我們用這個 module 提供的 relu 函數當作 activation function)\n",
    "\n",
    "# 寫好 neural network 的 class\n",
    "class CIFAR_Net(nn.Module):\n",
    "    # constructor 中我們會透過 specify weights 來定義模型中各個 layers 的 nodes 的數量 \n",
    "    def __init__(self):\n",
    "        super(CIFAR_Net, self).__init__()\n",
    "        # fully connected layers (dense layers) 在 PyTorch 中稱為 nn.Linear(目前layer的nodes數, 下一層layer的nodes數)\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 500)    # 由於每張照片都是 32 * 32 且有 RGB 三原色，因此一張照片總共有 32 * 32 * 3 個資料點\n",
    "        self.fc2 = nn.Linear(500, 10)    # 500 代表 hidden layer 有 500 個 nodes， 10 代表輸出層共有 10 個 nodes (10 個類別)\n",
    "        \n",
    "    def forward(self, x):    # inputs 為 x\n",
    "        # apply all weights to the inputs\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e6a6e4",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2278ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import torch.optim as optim    # 這個套件包含許多最佳化的方法\n",
    "\n",
    "# 建立模型物件\n",
    "net = CIFAR_Net()\n",
    "\n",
    "# 建立 cross entropy loss function 的物件\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 設定 optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr = 3e-4)\n",
    "\n",
    "for epoch in range(10):    # 所有的資料要被訓練10次 (10 個 epochs)\n",
    "    for i, data in enumerate(trainloaderCIFAR, start = 0):    # 每個 batch 之後都會更新 weights (i 為 batch number, data 為每個 batch 的資料集)\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 32 * 32 * 3)    # 將所有的資料點變成一維向量\n",
    "        \n",
    "        # 將 optimizer 的 gradient 歸零，以避免迴圈前一次的 gradient 的影響。每一次更新 weights 的 gradients 都是重新開始算的\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation (計算 output)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # optimisation\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()    # 計算 loss 這個函數中各個 weights 的 gradients\n",
    "        \n",
    "        # 利用計算得到的 gradients 更新 weights\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d39c82",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "af7582ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CIFAR-10  testing set accuracy of the network if: 51 %\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0, 0\n",
    "predictions = []\n",
    "\n",
    "# 將 net 這個 Net 的物件設定為 evaluation mode\n",
    "net.eval()\n",
    "\n",
    "for i, data in enumerate(testloaderCIFAR, start = 0):\n",
    "    # get the testing data\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.view(-1, 32 * 32 * 3)\n",
    "    \n",
    "    # 預測結果 (計算 output scores for each class)\n",
    "    outputs = net(inputs)\n",
    "    \n",
    "    # 將 output scores 轉換成類別 (誰的 output score 最大就屬於哪個類別)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # 將預測的類別轉換成 list\n",
    "    predictions.append(outputs)\n",
    "    \n",
    "    # 計算資料的總數\n",
    "    total += labels.size(0)\n",
    "    \n",
    "    # 計算預測正確的數量\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    \n",
    "# 印出預測正確率    \n",
    "print(\"The CIFAR-10  testing set accuracy of the network if: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5798664",
   "metadata": {},
   "source": [
    "預測正確率為 53%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d44d0",
   "metadata": {},
   "source": [
    "## 再用 MNIST 這個資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa72911",
   "metadata": {},
   "source": [
    "### 建立模型 (設計模型的 class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c10f29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class Net\n",
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self):    \n",
    "    \t# Define all the parameters of the net\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28 * 1, 200)    # 黑白照片，解析度 28 * 28\n",
    "        self.fc2 = nn.Linear(200, 10)    # hidden layer 200 nodes，output layer 10 nodes (0 ~ 9 個數字)\n",
    "\n",
    "    def forward(self, x):   \n",
    "    \t# Do the forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3cd7fa",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ecc01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Adam optimizer and Cross-Entropy loss function\n",
    "model = MNIST_Net()   \n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "for batch_idx, data in enumerate(trainloaderMNIST):\n",
    "    indenpendent_variables = data[0]    \n",
    "    labels = data[1]\n",
    "    indenpendent_variables = indenpendent_variables.view(-1, 28 * 28)\n",
    "    \n",
    "    # reset gradients in optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Complete a forward pass\n",
    "    output = model(indenpendent_variables)\n",
    "\n",
    "    # Compute the loss, gradients and change the weights\n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26be092",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a97e72c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing set accuracy of the network is: 73 %\n"
     ]
    }
   ],
   "source": [
    "# Set the model in eval mode\n",
    "model.eval()\n",
    "\n",
    "for batch_idx, data in enumerate(testloaderMNIST, 0):\n",
    "    independent_variables, labels = data\n",
    "    \n",
    "    # Put each image into a vector\n",
    "    independent_variables = independent_variables.view(-1, 28 * 28 * 1)\n",
    "    \n",
    "    # Do the forward pass and get the predictions\n",
    "    outputs = model(independent_variables)\n",
    "    \n",
    "    # 將 predicted scores 轉換成類別\n",
    "    _, outputs = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # 計算資料總數以及預測正確的資料數\n",
    "    total += labels.size(0)\n",
    "    correct += (outputs == labels).sum().item()\n",
    "    \n",
    "print('The testing set accuracy of the network is: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e75152",
   "metadata": {},
   "source": [
    "預測正確率為 73%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
