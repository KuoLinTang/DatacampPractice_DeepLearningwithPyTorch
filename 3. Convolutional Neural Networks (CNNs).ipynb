{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839b85f6",
   "metadata": {},
   "source": [
    "# Convolution operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e793865",
   "metadata": {},
   "source": [
    "Image Processing 時，單純使用 fully connected networks (dense networks) 有許多問題。\n",
    "\n",
    "1. 是否需要考慮各變數 (features) 之間的所有關係：一張照片的最左上角和最右下角的 pixels 之間可能完全沒有關係，因此這兩個 inputs 的 weights 不需要連到同一個 hidden layer 的 node。因此，使用 dense networks 會產生過多多餘的 weights，造成計算的成本增加以及耗時太久。\n",
    "2. 太多 weights 和 bias，可能造成 overfitting。\n",
    "\n",
    "因此，hidden layers 的 nodes 只需要連到前一個 layer 的幾個 nodes (相鄰的 pixels) 即可。現今最常用的架構就是 Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489aaf49",
   "metadata": {},
   "source": [
    "## Implementing CNN in PyTorch\n",
    "\n",
    "有兩種方式：\n",
    "1. 物件導向的方式建立一個 network 的 class (使用 torch.nn) (下圖左)\n",
    "2. Functional (使用 torch.nn.functional) (下圖右)\n",
    "\n",
    "![](Image/Image12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5170b60",
   "metadata": {},
   "source": [
    "### 實際寫法的差異\n",
    "\n",
    "**僅使用一個 kernel**\n",
    "![](Image/Image13.jpg)\n",
    "\n",
    "**使用五個 kernel**\n",
    "![](Image/Image14.jpg)\n",
    "\n",
    "in_channels = 3 代表輸入的照片有 RGB 三張圖層。 <br/>\n",
    "out_channels = 1 代表輸出的 feature map 只有一張，也就代表只有使用一個 kernel。 <br/>\n",
    "out_channels = 5 代表輸出的 feature map 有五張，也就代表有使用五個 kernels。 <br/>\n",
    "stride 代表 kernel 移動的速度。 <br/>\n",
    "padding 代表 kernel 是否要超出照片的邊緣，可以讓 kernel 掃出來的 feature maps 不會變小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee9860",
   "metadata": {},
   "source": [
    "# Pooling operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4567c381",
   "metadata": {},
   "source": [
    "![](Image/Image15.jpg)\n",
    "\n",
    "如上圖，假設有一組 64 個經過 kernels 掃出來的 feature maps， **Pooling** 就是將每個 feature maps 的解析度減少 (typically 長寬都變一半)。\n",
    "\n",
    "Pooling 可以讓訓練過程更有效率，也可以讓模型較不 overfitting (參數減少)。\n",
    "\n",
    "---\n",
    "\n",
    "有兩種常用的 pooling：\n",
    "1. Max pooling\n",
    "\n",
    "![](Image/Image16.jpg)\n",
    "\n",
    "2. Average pooling\n",
    "\n",
    "![](Image/Image17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad8fa28",
   "metadata": {},
   "source": [
    "## Max-pooling in PyTorch\n",
    "\n",
    "一樣有兩種方式：\n",
    "1. 物件導向的方式建立一個 network 的 class (使用 torch.nn) (下圖左)\n",
    "2. Functional (使用 torch.nn.functional) (下圖右)\n",
    "\n",
    "![](Image/Image18.jpg)\n",
    "\n",
    "Tensor 被四個中括號包含，代表 image 有四個 dimensions (minibatch size, depth, height, width)\n",
    "\n",
    "**torch.nn.MaxPool2d(2)** 以及 **F.max_pool2d(im, 2)** 代表將整個 image 分成許多 2 * 2 的格子，再取每個 2 * 2 的格子的 max 值來完成 pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f2ede",
   "metadata": {},
   "source": [
    "## Average-pooling in PyTorch\n",
    "\n",
    "一樣有兩種方式：\n",
    "1. 物件導向的方式建立一個 network 的 class (使用 torch.nn) (下圖左)\n",
    "2. Functional (使用 torch.nn.functional) (下圖右)\n",
    "\n",
    "![](Image/Image19.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0bc5b",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "**AlexNet 架構**\n",
    "\n",
    "![](Image/Image20.jpg)\n",
    "\n",
    "CNNs 其實就是包含許多 convolutional kernels 、 pooling layers 以及一些 dense layers 的 neural networks。上圖就有 5 個 convolutional layers、3 個 max-pooling layers、1 個 average-pooling layer、最後還有 3 個 fully connected layers，並且可以將 images 分成 1000 個不同的 classes。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea74423",
   "metadata": {},
   "source": [
    "## 建立 AlexNet 架構 in PyTorch\n",
    "\n",
    "![](Image/Image21.jpg)\n",
    "\n",
    "256 * 6 * 6 代表輸入的 channels 有 256 個，而每個 channel 的解析度都是 6 * 6。\n",
    "\n",
    "![](Image/Image22.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918a351",
   "metadata": {},
   "source": [
    "### 實際操作\n",
    "\n",
    "Convolutional Layers 的 channels 數量是沒有固定的 (hyper parameter)，可嘗試不同的數量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b9fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# define the CNN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Instantiate the ReLU nonlinearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Instantiate two convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Instantiate a max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Instantiate a fully connected layer\n",
    "        self.fc = nn.Linear(7 * 7 * 10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Prepare the image for the fully connected layer\n",
    "        x = x.view(-1, 7 * 7 * 10)\n",
    "\n",
    "        # Apply the fully connected layer and return the result\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e0dc1f",
   "metadata": {},
   "source": [
    "# Training Convolutional Neural Networks (CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3286f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import torch\n",
    "import torchvision    # a package which deals with datasets and pre-trained neural networks\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e7bab",
   "metadata": {},
   "source": [
    "## Create training dataloader and testing dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0264927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# define a transformation of images to torch tensors using torchvision transforms \n",
    "# 下載資料時就可以直接用這個定義好的 transform 物件來轉換資料\n",
    "transformCIFAR = transforms.Compose(\n",
    "    [transforms.ToTensor(),    # define a transformation of images object\n",
    "    transforms.Normalize((0.4914, 0.48216, 0.44653),    # R, G, B 的平均 (pre-computed) 用來將數據標準化\n",
    "                        (0.24703, 0.24349, 0.26159))]    # R, G, B 的標準差 (pre-computed) 用來將數據標準化\n",
    ")\n",
    "\n",
    "# download the CIFAR-10 datasets\n",
    "training_setCIFAR = torchvision.datasets.CIFAR10(root = \"./Datasets/CIFAR-10\", train = True, download = True, transform = transformCIFAR)\n",
    "testing_setCIFAR = torchvision.datasets.CIFAR10(root = \"./Datasets/CIFAR-10\", train = False, download = True, transform = transformCIFAR)\n",
    "\n",
    "# load the CIFAR-10 datasets\n",
    "trainloaderCIFAR = torch.utils.data.DataLoader(training_setCIFAR, batch_size = 32, shuffle = True, num_workers = 4)\n",
    "testloaderCIFAR = torch.utils.data.DataLoader(testing_setCIFAR, batch_size = 32, shuffle = False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d95c78",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0818b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN model\n",
    "class CIFAR_Net(nn.Module):\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(CIFAR_Net, self).__init__()\n",
    "        \n",
    "        # Instantiate the ReLU nonlinearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Instantiate two convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Instantiate a max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)    # pooling 會被用在每個 convolutional layer 之後 (所以會用 3 次)\n",
    "        \n",
    "        # Instantiate a fully connected layer\n",
    "        self.fc = nn.Linear(4 * 4 * 128, num_classes)    # pooling 3 次所以解析度變成 4 * 4 (32 / 2 --> 16 / 2 --> 8 / 2 --> 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Prepare the image for the fully connected layer\n",
    "        x = x.view(-1, 4 * 4 * 128)\n",
    "\n",
    "        # Apply the fully connected layer and return the result\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4458a5",
   "metadata": {},
   "source": [
    "## Define the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc6a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立模型\n",
    "net = CIFAR_Net()\n",
    "\n",
    "# 建立 cross entropy loss function 的物件\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 設定 optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr = 3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f54200b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f20e54ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TangKuoLin\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:147: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:115.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Training!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):    # 所有的資料要被訓練10次 (10 個 epochs)\n",
    "    for i, data in enumerate(trainloaderCIFAR, start = 0):    # 每個 batch 之後都會更新 weights (i 為 batch number, data 為每個 batch 的資料集)\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # 將 optimizer 的 gradient 歸零，以避免迴圈前一次的 gradient 的影響。每一次更新 weights 的 gradients 都是重新開始算的\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation (計算 output)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # optimisation\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()    # 計算 loss 這個函數中各個 weights 的 gradients\n",
    "        \n",
    "        # 利用計算得到的 gradients 更新 weights\n",
    "        optimizer.step()\n",
    "        \n",
    "print(\"Finish Training!\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e95a18",
   "metadata": {},
   "source": [
    "## Evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c903835d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CIFAR-10  testing set accuracy of the network if: 75 %\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0, 0\n",
    "predictions = []\n",
    "\n",
    "# 將 net 這個 Net 的物件設定為 evaluation mode\n",
    "net.eval()\n",
    "\n",
    "for i, data in enumerate(testloaderCIFAR, start = 0):\n",
    "    # get the testing data\n",
    "    inputs, labels = data\n",
    "    \n",
    "    # 預測結果 (計算 output scores for each class)\n",
    "    outputs = net(inputs)\n",
    "    \n",
    "    # 將 output scores 轉換成類別 (誰的 output score 最大就屬於哪個類別)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # 將預測的類別轉換成 list\n",
    "    predictions.append(outputs)\n",
    "    \n",
    "    # 計算資料的總數\n",
    "    total += labels.size(0)\n",
    "    \n",
    "    # 計算預測正確的數量\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    \n",
    "# 印出預測正確率    \n",
    "print(\"The CIFAR-10 testing set accuracy of the network if: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da9850",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e6615",
   "metadata": {},
   "source": [
    "# Training Convolutional Neural Networks (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc7de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import torch\n",
    "import torchvision    # a package which deals with datasets and pre-trained neural networks\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27a3a46",
   "metadata": {},
   "source": [
    "## Create training dataloader and testing dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a0cec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TangKuoLin\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Transform the data to torch tensors and normalize it \n",
    "transformMNIST = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307), ((0.3081)))])   # 由於 MNIST 是灰階，因此沒有像 RGB 一樣三個平均和三個標準差，這裡平均為 0.1307，標準差為 0.3081\n",
    "\n",
    "# Prepare training set and testing set\n",
    "trainsetMNIST = torchvision.datasets.MNIST(root = \"./Datasets/MNIST\", train=True, \n",
    "                                      download=True, transform=transformMNIST)\n",
    "testsetMNIST = torchvision.datasets.MNIST(root = \"./Datasets/MNIST\", train=False, \n",
    "                                     download=True, transform=transformMNIST)\n",
    "\n",
    "# Prepare training loader and testing loader\n",
    "trainloaderMNIST = torch.utils.data.DataLoader(trainsetMNIST, batch_size = 32,\n",
    "                                          shuffle = True, num_workers=8)\n",
    "testloaderMNIST = torch.utils.data.DataLoader(testsetMNIST, batch_size = 32,\n",
    "                                         shuffle = False, num_workers=8) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf31d3",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c768d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN model\n",
    "class MNIST_Net(nn.Module):\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(MNIST_Net, self).__init__()\n",
    "        \n",
    "        # Instantiate the ReLU nonlinearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Instantiate two convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Instantiate a max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)    # pooling 會被用在每個 convolutional layer 之後 (所以會用 3 次)\n",
    "        \n",
    "        # Instantiate a fully connected layer\n",
    "        self.fc = nn.Linear(7 * 7 * 64, num_classes)    # pooling 2 次所以解析度變成 7 * 7 (28 / 2 --> 14 / 2 --> 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Prepare the image for the fully connected layer\n",
    "        x = x.view(-1, 7 * 7 * 64)\n",
    "\n",
    "        # Apply the fully connected layer and return the result\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95db1395",
   "metadata": {},
   "source": [
    "## Define the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "068071da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立模型\n",
    "net = MNIST_Net()\n",
    "\n",
    "# 建立 cross entropy loss function 的物件\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 設定 optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr = 3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777e5b3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c16be2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Training!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):    # 所有的資料要被訓練10次 (10 個 epochs)\n",
    "    for i, data in enumerate(trainloaderMNIST, start = 0):    # 每個 batch 之後都會更新 weights (i 為 batch number, data 為每個 batch 的資料集)\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # 將 optimizer 的 gradient 歸零，以避免迴圈前一次的 gradient 的影響。每一次更新 weights 的 gradients 都是重新開始算的\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation (計算 output)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # optimisation\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()    # 計算 loss 這個函數中各個 weights 的 gradients\n",
    "        \n",
    "        # 利用計算得到的 gradients 更新 weights\n",
    "        optimizer.step()\n",
    "        \n",
    "print(\"Finish Training!\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae9c75",
   "metadata": {},
   "source": [
    "## Evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8fe7311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MNIST testing set accuracy of the network if: 98 %\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0, 0\n",
    "predictions = []\n",
    "\n",
    "# 將 net 這個 Net 的物件設定為 evaluation mode\n",
    "net.eval()\n",
    "\n",
    "for i, data in enumerate(testloaderMNIST, start = 0):\n",
    "    # get the testing data\n",
    "    inputs, labels = data\n",
    "    \n",
    "    # 預測結果 (計算 output scores for each class)\n",
    "    outputs = net(inputs)\n",
    "    \n",
    "    # 將 output scores 轉換成類別 (誰的 output score 最大就屬於哪個類別)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # 將預測的類別轉換成 list\n",
    "    predictions.append(outputs)\n",
    "    \n",
    "    # 計算資料的總數\n",
    "    total += labels.size(0)\n",
    "    \n",
    "    # 計算預測正確的數量\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    \n",
    "# 印出預測正確率    \n",
    "print(\"The MNIST testing set accuracy of the network if: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e4d69",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks 的預測結果遠高於 fully connected networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
